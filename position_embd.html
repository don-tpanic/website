<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Giving LLMs too much RoPE: A limit on Sutton’s Bitter Lesson</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<style>
			/* Figure styling */
			.fig {
				text-align: center;
				margin: 2em 0 3em 0; /* Increased bottom margin for better spacing */
			}
			
			.fig img {
				display: block;
				margin: 0 auto;
				max-width: 100%;
				height: auto;
			}
			
			/* Image size controls */
			.img-large { max-width: 100% !important; }
			.img-medium { max-width: 70% !important; }
			.img-small { max-width: 55% !important; }
			.img-tiny { max-width: 30% !important; }
			
			/* Caption styling */
			.fig figcaption {
				text-align: center;
				margin-top: 1em;
				padding: 0 1em;
			}
			
			.inner-caption {
				text-align: center;
				font-style: italic;
				color: #666;
				line-height: 1.4;
			}
			
			/* Caption width controls */
			.caption-full { max-width: 100%; margin: 0 auto; }
			.caption-wide { max-width: 85%; margin: 0 auto; }
			.caption-medium { max-width: 70%; margin: 0 auto; }
			.caption-narrow { max-width: 55%; margin: 0 auto; }
		</style>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Preliumtarn</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="cp_more.html" class="active">Read more</a></li>
						<!-- <li><a href="elements.html">Elements</a></li> -->
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Giving LLMs too much RoPE: A limit on Sutton’s Bitter Lesson</h1>

							<h2>Introduction</h2>
							<p>Sutton's Bitter Lesson (<a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">Sutton, 2019</a>) argues that machine learning breakthroughs, like AlphaGo, BERT, and large-scale vision models, rely on general, computation-driven methods that prioritize learning from data over human-crafted priors. Large language models (LLMs) based on transformer architectures exemplify this trend, scaling effectively with data and compute. Yet, positional embeddings—a key transformer component—seem to challenge this philosophy. Most embedding schemes are fixed, not learned, and embed a human-designed prior that words closer in a sentence are more relevant than those farther apart. This blog explores this machine learning practice that appears to defy the Bitter Lesson. We also analyze patterns in learned absolute positional embeddings, which partially align with fixed, human-designed schemes but show intriguing variations, highlighting the complexity of positional encodings in LLMs and the need for further research.</p>

							<h3>Why Transformers Need Positional Embeddings</h3>
							<p>Transformers process tokens in parallel using permutation-invariant attention mechanisms, lacking inherent sequence awareness. Without positional information, they treat "The cat sat on the mat" and "Mat the on sat cat the" identically, despite order being critical for meaning in language. While LLMs may learn word order differently from humans, they require consistent order encoding to function (see our <a href="#">recent blog</a> for more). Positional embeddings provide this order, using either fixed or learned methods. Human intuition suggests nearby words are more relevant than distant ones, implying a decay in influence over distance—a prior often baked into positional encoding designs. However, <a href="https://arxiv.org/abs/2410.21216v2">Chen et al., 2024</a> argue this assumption may be outdated for modern LLMs.</p>

							<h2>A Brief History of Positional Embeddings</h2>

							<h3>Early Positional Embeddings: Baking in Long-Term Decay</h3>
							<p>The original Transformer (<a href="https://arxiv.org/abs/1706.03762">Vaswani et al., 2017</a>) used sinusoidal positional embeddings, applying deterministic sinusoidal functions to encode positions. These embeddings exhibit long-term decay, where similarity between embeddings decreases with token distance, aligning with the intuition that distant tokens are less relevant (Fig. 1).</p>

							<figure class="fig">
								<img src="images/blog/sinusoidal.png" title="Figure 1" class="img-small" alt="Sinusoidal positional embeddings similarity">
								<figcaption>
									<div class="inner-caption caption-medium">
										<strong>Figure 1: Cosine similarity of sinusoidal positional embeddings, showing decay in similarity as token distance increases, reflecting the intuition that nearby tokens are more relevant.</strong>
									</div>
								</figcaption>
							</figure>

							<h3>Absolute Learnable Embeddings: Data-Driven but Limited</h3>
							<p>Absolute learnable positional embeddings, used in models like BERT, GPT-1, GPT-2, Galactica, and OPT, align with Sutton's Bitter Lesson by assigning trainable vectors to each position, allowing the model to learn positional relationships from data. This data-driven approach avoids human priors, theoretically enabling optimal patterns to emerge during training. However, these embeddings are limited by a fixed maximum sequence length, hindering generalization to longer contexts.</p>

							<h3>Current Generation Embeddings: Back to Human Priors</h3>
							<p>State-of-the-art models like LLaMA, Qwen, and DeepSeek use Rotary Position Embeddings (RoPE) (<a href="https://arxiv.org/abs/2104.09864">Su et al., 2021</a>). RoPE applies fixed, relative rotations in attention, reintroducing a human prior of long-term decay (Fig. 2) while enabling length extrapolation—a key advantage for modern LLMs. This shift seems to step back from the data-driven ideal, suggesting that learning from data may not always be optimal, especially given practical constraints like context length.</p>

							<figure class="fig">
								<img src="images/blog/rope_decay.png" title="Figure 2" class="img-small" alt="RoPE similarity decay">
								<figcaption>
									<div class="inner-caption caption-medium">
										<strong>Figure 2: RoPE similarity decay, showing decreasing similarity with increasing token distance, enabling effective handling of long sequences with built-in decay.</strong>
									</div>
								</figcaption>
							</figure>

							<h2>Absolute Positional Embeddings: Revisited</h2>
							<p>Why did the field shift from human-designed fixed positional embeddings to a data-driven approach consistent with Sutton's Bitter Lesson, only to return to fixed schemes? To explore this, we investigate learnable absolute positional embeddings, uncovering patterns that warrant further study.</p>

							<p>Across various model sizes, architectures, and training datasets, these embeddings partially converge on the long-term decay seen in fixed embeddings. Surprisingly, they also show periodic oscillations, a byproduct in fixed embeddings designed for decay but lacking clear theoretical justification. Their presence in learnable embeddings is puzzling, varying with model capacity, architecture, and training data. Are these oscillations beneficial or artifacts? Their variability underscores the need for further research to clarify their role and optimize positional encodings in LLMs.</p>

							<h3>GPT-2: A Periodic Surprise</h3>
							<p>We analyzed cosine similarities of positional embeddings in pretrained GPT-2 models (Fig. 3). The top panel shows pairwise similarities between token positions, and the bottom averages similarities by distance. A smooth, periodic pattern emerges, noted in <a href="https://arxiv.org/abs/2010.04903">research</a> and <a href="https://www.lesswrong.com/posts/qvWP3aBDBaqXvPNhS/gpt-2-s-positional-embedding-matrix-is-a-helix">blogs</a>, but without clear explanation. One might assume models capture hierarchical structure in training data, but this doesn't hold: the pattern persists across diverse datasets without aligned peaks. Why does a data-driven approach produce such structured patterns?</p>

							<figure class="fig">
								<img src="images/blog/positional_embedding_similarity_gpt2_pretrained.png" title="Figure 3" class="img-small" alt="GPT-2 positional embedding similarities pairwise">
								<img src="images/blog/positional_embedding_similarity_by_distance_gpt2_pretrained.png" title="Figure 3" class="img-small" alt="GPT-2 positional embedding similarities by distance">
								<figcaption>
									<div class="inner-caption caption-wide">
										<strong>Figure 3: Cosine similarities of GPT-2 pretrained positional embeddings, showing periodic oscillations (top: pairwise similarities; bottom: averaged by distance), defying expected monotonic decay.</strong>
									</div>
								</figcaption>
							</figure>

							<h3>Varying Patterns Across Models</h3>
							<p>We examined Galactica (125M, 1.3B, 6.7B), trained on academic papers, and OPT (125M, 350M, 1.3B, 2.7B, 6.7B), trained on general text. Galactica's smaller models show periodicity, but the 6.7B model trends toward a simpler similarity decrease (Fig. 4). OPT models vary: the 350M model mirrors GPT-2's periodicity, while the 125M and larger models diverge, losing periodic structure (Fig. 5).</p>

							<figure class="fig">
								<img src="images/blog/positional_embedding_similarity_galactica.png" title="Figure 4" class="img-small" alt="Galactica positional embedding similarities pairwise">
								<img src="images/blog/positional_embedding_similarity_by_distance_galactica.png" title="Figure 4" class="img-small" alt="Galactica positional embedding similarities by distance">
								<figcaption>
									<div class="inner-caption caption-wide">
										<strong>Figure 4: Galactica pretrained positional embedding similarities, with smaller models showing periodicity and the 6.7B model trending toward monotonic decay (top: pairwise; bottom: by distance).</strong>
									</div>
								</figcaption>
							</figure>

							<figure class="fig">
								<img src="images/blog/positional_embedding_similarity_opt.png" title="Figure 5" class="img-medium" alt="OPT positional embedding similarities pairwise">
								<img src="images/blog/positional_embedding_similarity_by_distance_opt.png" title="Figure 5" class="img-small" alt="OPT positional embedding similarities by distance">
								<figcaption>
									<div class="inner-caption caption-wide">
										<strong>Figure 5: OPT pretrained positional embedding similarities, with the 350M model showing periodicity similar to GPT-2, while others vary (top: pairwise; bottom: by distance).</strong>
									</div>
								</figcaption>
							</figure>

							<h3>Training Data's Role</h3>
							<p>We analyzed CodeParrot (110M, 1.5B), GPT-2-based models trained on Python code. The 110M model shows periodicity distinct from GPT-2's 124M, and the 1.5B model diverges further, unique from both smaller CodeParrot and same-sized GPT-2 models (Fig. 6). This suggests training data shapes these patterns.</p>

							<figure class="fig">
								<img src="images/blog/positional_embedding_similarity_codeparrot.png" title="Figure 6" class="img-tiny" alt="CodeParrot positional embedding similarities pairwise">
								<img src="images/blog/positional_embedding_similarity_by_distance_codeparrot.png" title="Figure 6" class="img-small" alt="CodeParrot positional embedding similarities by distance">
								<figcaption>
									<div class="inner-caption caption-wide">
										<strong>Figure 6: CodeParrot pretrained positional embedding similarities, showing distinct periodic patterns influenced by code-specific training data (top: pairwise; bottom: by distance).</strong>
									</div>
								</figcaption>
							</figure>

							<p>We also studied GPT-2 124M variants trained on neuroscience articles in forward (FWD), backward (BWD), and permuted (PERM) orders, detailed in a <a href="#">blog</a> and <a href="https://arxiv.org/abs/2505.08739">paper</a>. These show a weak similarity decrease up to 450 tokens, dipping negative before returning to zero, distinct from other models and a random baseline (INIT) (Fig. 7).</p>

							<figure class="fig">
								<img src="images/blog/positional_embedding_similarity_x_models_reordered.png" title="Figure 7" class="img-small" alt="GPT-2 reordered training positional embedding similarities pairwise">
								<img src="images/blog/positional_embedding_similarity_by_distance_x_models_reordered.png" title="Figure 7" class="img-small" alt="GPT-2 reordered training positional embedding similarities by distance">
								<figcaption>
									<div class="inner-caption caption-wide">
										<strong>Figure 7: GPT-2 124M positional embedding similarities trained on neuroscience text in forward (FWD), backward (BWD), and permuted (PERM) orders, compared to random initialization (INIT), showing weak decay up to 450 tokens (top: pairwise; bottom: by distance).</strong>
									</div>
								</figcaption>
							</figure>

							<h2>A Frontier to Sutton's Bitter Lesson</h2>
							<p>The shift from absolute learnable positional embeddings to RoPE in modern LLMs highlights a trade-off between Sutton's data-driven ideal and practical scalability. Learnable embeddings align with the Bitter Lesson but are limited by fixed context lengths, while RoPE's fixed decay enables length extrapolation at the cost of reintroducing human priors. Both fixed (e.g., sinusoidal) and learnable embeddings exhibit periodic oscillations, but in fixed embeddings, these are a byproduct, with decay as the intended design. Their purpose remains unclear.</p>

							<p>Intriguingly, learnable embeddings' oscillations suggest partial convergence with fixed methods, but models like GPT-2 (all sizes) and OPT-350M show exaggerated oscillations, raising concerns about potential degenerative solutions or training artifacts. In contrast, models like Galactica-6.7B and larger OPT (and the smallest OPT) variants display more desirable decay with oscillations that adapt to training data and model scale. This flexibility, absent in RoPE's rigid structure, may better capture nuanced patterns but risks instability. Are these oscillations beneficial or artifacts? They are unlikely to capture hierarchical structure in text, as peaks and troughs do not align across model sizes and architectures, but the periodicity may help distinguish positions, especially at greater distances.</p>
						</div>
					
						<div class="inner">
							<div class="post-info">
							<p><strong>Authors:</strong> Xiaoliang Luo and Bradley C. Love</p>
							<p><strong>Date:</strong> June 12, 2025</p>
							<p><strong>Note:</strong> Also posted on <a href="https://bradlove.org/blog/position-embd">https://bradlove.org/blog/position-embd</a></p>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					
					<ul class="menu">
						<li>&copy; LXL. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>